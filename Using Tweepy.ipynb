{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy \n",
    "import pandas as pd\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Add your credentials here\n",
    "Path = 'D:/Research Assistant/ResearchAssisstant(Megan)/Twitter Project/API_Credientials.xlsx'\n",
    "\n",
    "Credientials = pd.read_excel(Path)\n",
    "\n",
    "consumer_key = Credientials['API key'][0]\n",
    "consumer_secret = Credientials['API secret key'][0]\n",
    "Access_token = Credientials['Access token'][0]\n",
    "Access_token_secret = Credientials['Access token secret'][0]\n",
    "\n",
    "# print(API_secret_key)\n",
    "\n",
    "\n",
    "# initialize api instance\n",
    "# twitter_api = twitter.Api(consumer_key=API_key,\n",
    "#                         consumer_secret=API_secret_key,\n",
    "#                         access_token_key=Access_token,\n",
    "#                         access_token_secret=Access_token_secret)\n",
    "# # test authentication\n",
    "# print(twitter_api.VerifyCredentials())\n",
    "\n",
    "# twitter_keys = {\n",
    "#         'consumer_key':        API_key,\n",
    "#         'consumer_secret':     API_secret_key,\n",
    "#         'access_token_key':    Access_token,\n",
    "#         'access_token_secret': Access_token_secret\n",
    "#     }\n",
    "# print(twitter_keys['consumer_secret'])\n",
    "# #Setup access to API\n",
    "# auth = tweepy.OAuthHandler(twitter_keys['consumer_key'], twitter_keys['consumer_secret'])\n",
    "# auth.set_access_token(twitter_keys['access_token_key'], twitter_keys['access_token_secret'])\n",
    "\n",
    "# api = tweepy.API(auth)\n",
    "\n",
    "# #Make call on home timeline, print each tweets text\n",
    "# public_tweets = api.home_timeline()\n",
    "# for tweet in public_tweets:\n",
    "#     print(tweet.text)\n",
    "\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(Access_token, Access_token_secret)\n",
    "api = tweepy.API(auth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-00ff21ef0786>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;31m# save the id of the oldest tweet less one\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[0moldest\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0malltweets\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mid\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;31m# keep grabbing tweets until there are no tweets left to grab\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "# initialize a list to hold all the tweepy Tweets\n",
    "alltweets = []\n",
    "\n",
    "# make initial request for most recent tweets (200 is the maximum allowed count)\n",
    "new_tweets = api.user_timeline( count=200)\n",
    "\n",
    "# save most recent tweets\n",
    "alltweets.extend(new_tweets)\n",
    "\n",
    "# save the id of the oldest tweet less one\n",
    "oldest = alltweets[-1].id - 1\n",
    "\n",
    "# keep grabbing tweets until there are no tweets left to grab\n",
    "while len(new_tweets) > 0:\n",
    "    print\n",
    "    \"getting tweets before %s\" % (oldest)\n",
    "\n",
    "    # all subsiquent requests use the max_id param to prevent duplicates\n",
    "    new_tweets = api.user_timeline( count=200, max_id=oldest)\n",
    "\n",
    "    # save most recent tweets\n",
    "    alltweets.extend(new_tweets)\n",
    "\n",
    "    # update the id of the oldest tweet less one\n",
    "    oldest = alltweets[-1].id - 1\n",
    "\n",
    "    print\n",
    "    \"...%s tweets downloaded so far\" % (len(alltweets))\n",
    "\n",
    "# transform the tweepy tweets into a 2D array that will populate the csv\n",
    "outtweets = [[tweet.id_str, tweet.created_at, tweet.text.encode(\"utf-8\")] for tweet in alltweets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scraptweets(search_words, date_since, numTweets, numRuns):\n",
    "    \n",
    "    # Define a for-loop to generate tweets at regular intervals\n",
    "    # We cannot make large API call in one go. Hence, let's try T times\n",
    "    \n",
    "    # Define a pandas dataframe to store the date:\n",
    "    db_tweets = pd.DataFrame(columns = ['username', 'acctdesc', 'location', 'following',\n",
    "                                        'followers', 'totaltweets', 'usercreatedts', 'tweetcreatedts',\n",
    "                                        'retweetcount', 'text', 'hashtags']\n",
    "                                )\n",
    "    program_start = time.time()\n",
    "    for i in range(0, numRuns):\n",
    "        # We will time how long it takes to scrape tweets for each run:\n",
    "        start_run = time.time()\n",
    "        \n",
    "        # Collect tweets using the Cursor object\n",
    "        # .Cursor() returns an object that you can iterate or loop over to access the data collected.\n",
    "        # Each item in the iterator has various attributes that you can access to get information about each tweet\n",
    "        tweets = tweepy.Cursor(api.search, q=search_words, lang=\"en\", since=date_since, tweet_mode='extended').items(numTweets)# Store these tweets into a python list\n",
    "        tweet_list = [tweet for tweet in tweets]# Obtain the following info (methods to call them out):\n",
    "        # user.screen_name - twitter handle\n",
    "        # user.description - description of account\n",
    "        # user.location - where is he tweeting from\n",
    "        # user.friends_count - no. of other users that user is following (following)\n",
    "        # user.followers_count - no. of other users who are following this user (followers)\n",
    "        # user.statuses_count - total tweets by user\n",
    "        # user.created_at - when the user account was created\n",
    "        # created_at - when the tweet was created\n",
    "        # retweet_count - no. of retweets\n",
    "        # (deprecated) user.favourites_count - probably total no. of tweets that is favourited by user\n",
    "        # retweeted_status.full_text - full text of the tweet\n",
    "        # tweet.entities['hashtags'] - hashtags in the tweet# Begin scraping the tweets individually:\n",
    "        noTweets = 0\n",
    "        for tweet in tweet_list:# Pull the values\n",
    "            username = tweet.user.screen_name\n",
    "            acctdesc = tweet.user.description\n",
    "            location = tweet.user.location\n",
    "            following = tweet.user.friends_count\n",
    "            followers = tweet.user.followers_count\n",
    "            totaltweets = tweet.user.statuses_count\n",
    "            usercreatedts = tweet.user.created_at\n",
    "            tweetcreatedts = tweet.created_at\n",
    "            retweetcount = tweet.retweet_count\n",
    "            hashtags = tweet.entities['hashtags']\n",
    "            try:\n",
    "                text = tweet.retweeted_status.full_text\n",
    "            except AttributeError:  # Not a Retweet\n",
    "                text = tweet.full_text# Add the 11 variables to the empty list - ith_tweet:\n",
    "            ith_tweet = [username, acctdesc, location, following, followers, totaltweets,\n",
    "                         usercreatedts, tweetcreatedts, retweetcount, text, hashtags]# Append to dataframe - db_tweets\n",
    "            db_tweets.loc[len(db_tweets)] = ith_tweet# increase counter - noTweets  \n",
    "            noTweets += 1\n",
    "        \n",
    "        # Run ended:\n",
    "        end_run = time.time()\n",
    "        duration_run = round((end_run-start_run)/60, 2)\n",
    "        \n",
    "        print('no. of tweets scraped for run {} is {}'.format(i + 1, noTweets))\n",
    "        print('time take for {} run to complete is {} mins'.format(i+1, duration_run))\n",
    "        \n",
    "        time.sleep(920) #15 minute sleep time# Once all runs have completed, save them to a single csv file:\n",
    "    from datetime import datetime\n",
    "    \n",
    "    # Obtain timestamp in a readable format\n",
    "    to_csv_timestamp = datetime.today().strftime('%Y%m%d_%H%M%S')# Define working path and filename\n",
    "    path = os.getcwd()\n",
    "    filename = 'Data/TweetsWithTweepy.csv'# Store dataframe in csv with creation date timestamp\n",
    "    db_tweets.to_csv(filename, index = False)\n",
    "    \n",
    "    program_end = time.time()\n",
    "    print('Scraping has completed!')\n",
    "    print('Total time taken to scrap is {} minutes.'.format(round(program_end - program_start)/60, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no. of tweets scraped for run 1 is 58\n",
      "time take for 1 run to complete is 0.09 mins\n"
     ]
    }
   ],
   "source": [
    "# Initialise these variables:\n",
    "search_words = \"#Covid AND #dementia\"\n",
    "date_since = \"2020-08-28\"\n",
    "numTweets = 2500\n",
    "numRuns = 1# Call the function scraptweets\n",
    "scraptweets(search_words, date_since, numTweets, numRuns)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
